---
title: "Predicting_House_Prices_Using_Linear_Regression"
author: "Laura Cline"
date: "02/08/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries 

`MASS` library is a very large collection of datasets and functions. 
`ISLR` includes datasets from "Intro to Statistical Learning" 

```{r}
library(MASS)
library(ISLR)
set.seed(88)
```

```{r}
write.csv(Boston, "boston.csv")
```

# Simple Linear Regression 



```{r}
names(Boston)
?Boston
```

We will start by using the `lm()` function to fit a simple linear regression model, with `medv` (median value of owner-occupied homes in $1000s) as the response and `lstat` (lower status of the population as percent) as the predictor. The basic syntax is `lm(y~x, data)`, where `y` is the response, `x` is the predictor, and `data` is the dataset in which these two variables are kept. 

For more detailed information, we use `summary(lm.fit)`. This gives us the p-values and standard errors for the coefficients as well as the $R^2$ statistic and F-statistic for the model. 

```{r}
lm.fit = lm(medv ~ lstat, data = Boston)
summary(lm.fit)
```

We can use the `names()` function in order to find out what other pieces of information are stored in `lm.fit`. 

```{r}
names(lm.fit)
```

```{r}
coef(lm.fit)
```

The `predict()` function can be used to predict confidence intervals and prediction intervals for the prediction of `medv` for a given value of `lstat`. 

```{r}
predict(lm.fit, data.frame(lstat=c(5, 10, 15)), interval="confidence")
```
```{r}
predict(lm.fit, data.frame(lstat=c(5,10,15)), interval = "prediction")
```

For instance, the 95% confidence interval associated with a `lstat` value of 10 is (24.47, 25.63) and the 95% prediction interval is (12.82, 37.28). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of 25.05 for `medv` when `lstat` equals 10), but the latter is substantially wider. 

We will now plot `medv` and `lstat` along with the least squares regression line using the `plot()` and `abline()` functions. 

```{r}
plot(Boston$lstat, Boston$medv)
abline(lm.fit)
```

There is some evidence for non-linearity in the relationship between `lstat` and `medv`. 

The `abline` function can be used to draw any line, not just the least squares regression line. To draw a line with an intercept `a` and slope `b`, we type `abline(a,b)`. Below we experiment with some additional setttings for plotting lines and points. The `lwd=3` command causes the width of the regression line to be increased by a factor of 3; this works for the `plot()` and `lines()` functions also. We can also use the `pch` option to create different plotting symbols.

```{r}
plot(Boston$lstat, Boston$medv, pch=1:20)
abline(lm.fit, lwd=3, col="red")
```

Alternatively, we can compute the residuals from the linear regression fit using the `residuals()` function. The function `rstudent()` will return the studentized residuals, and we can use this function to plot the residuals against the fitted values. 

```{r}
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
```

On the basis of residuals plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the `hatvalues()` function. 

The `whuch.max()` function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic. 

```{r}
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

# Multiple Linear Regression 

In order to fit a multiple linear regression model using least squares, we again use the `lm()` function. The syntax `lm(y~x1+x2+x3)` is used to fit a model with three predictors, `x1`, `x2`, and `x3`. The `summary()` function now outputs the regression coefficients for all the predictors. 

```{r}
lm.fit = lm(medv ~ lstat+age, data=Boston)
summary(lm.fit)
```

The Boston dataset has 13 predictors. We can include all of them in the regression using the following shorthand:

```{r}
lm.fit = lm(medv~., data = Boston)
summary(lm.fit)
```

We can access the individual components of a summary object by name (type `?summary.lm` to see what is available). Hence `summary(lm.fit)$r.sq` gives us the $R^2$, and `summary(lm.fit)$sigma` gives us the RSE. The `vif()` function, part of the `car` package, can be used to compute variance inflation factors. Most VIFs are low to moderate for this data.

```{r}
library(car)
vif(lm.fit)
```

What id we would like to do regression using all of the variables but one For example, if we wanted to exclude `age`. 

```{r}
lm.fit1 = lm(medv~.-age, data=Boston)
summary(lm.fit1)
```

Alternatively, the `update()` function can be used. 

```{r}
lm.fit1 = update(lm.fit, ~.-age)
```

# Interaction Terms 

It's easy to include interaction terms in a linear model using the `lm()` function. The syntax `lstat:black` tells R to include an interaction term between `lstat` and `black`. The syntax `lstat*age` simultaneously includes `lstat`, `age` and the interaction term `lstat*age` as predictors; it is shorthand for `lstat+age+lstat:age`. 

```{r}
summary(lm(medv~lstat*age, data=Boston))
```

# Non-Linear Transformations of the Predictors

The `lm()` function can also accommodate non-linear transformations of the predictors. For instance, given a predictor X, we can create a predictor $X^2$ using `I(X^2)`. The function `I()` is needed since the ^ has a special meaning in a formula; wrapping as we do allows the standard usage in R, which is to raise `X` to the power of `2`. We now perform a regression of `medv` onto `lstat` and `lstat^2`. 

```{r}
lm.fit2 = lm(medv~lstat + I(lstat^2), data = Boston)
summary(lm.fit2)
```

The near-zero p-value associated with the quadratic term suggests that it leads to an improved model. We use `anova()` function to further quantify the extent to which the quadratic fit is superior to the linear fit. 

```{r}
lm.fit = lm(medv~lstat, data = Boston)
anova(lm.fit, lm.fit2)
```

Here, Model 1 represents the linear submodel containing only one predictor `lstat`, while Model 2 corresponds to the larger quadratic model that has two predictors `lstat` and `lstat^2`. The `anova()` function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here, the F-Statistic is 135.2 and the associated p-value is virtually zero. This provides clear evidence that the model containing the predictors `lstat` and `lstat^2` is far superior to the model that only contains the predictor `lstat`. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between `medv` and `lstat`. If we type:

```{r}
par(mfrow=c(2,2))
plot(lm.fit2)
```

then we see that when the `lstat^2` term is included in the model, there is a discernible pattern in the residuals. 

In order to create a cubic fit, we include a predictor of the form `I(X^3)`. However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the `poly()` function to create the polynomial within `lm()`. For example, the following command produces the fifth-order polynomial fit:

```{r}
lm.fit5 = lm(medv~poly(lstat, 5), data=Boston)
summary(lm.fit5)
```

This suggests that including additional polynomial terms, up to the fifth order, leads to an improvement in model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant p-values in a regression fit. 
Of course, we are not restricted to polynomial transformations of the predictors. Here we try a log transformation:

```{r}
summary(lm(medv~log(rm), data = Boston))
```

# Predict Per Capita Crime Rate 

```{r}
# Loop over each predictor and look for a statistically signficiant simple linear regression 
crim = Boston[,1]
model_f_value = c()
model_p_Value = c()
univariable_beta_value = c()
possible_predictors = colnames(Boston)

for(pi in 1:length(possible_predictors)) {
  if(possible_predictors[pi] == 'crim') {next}
  x = Boston[,pi]
  m = lm(crim ~ x, data = Boston)
  s = summary(m)
  model_f_value = c(model_f_value, s$fstatistic[1])
  model_p_Value = c(model_p_Value, anova(m)$'Pr(>F)'[1])
  univariable_beta_value = c(univariable_beta_value, coefficients(m)['x'])
  print(sprintf("%s %10.6f", possible_predictors[pi], coefficients(m)['x']))
}
```

```{r}
# Let's look at each models F-statistics:

DF = data.frame(feature=colnames(Boston)[-1], f_values=model_f_value, p_values = model_p_Value)
DF[order(model_f_value),]
```
```{r}
# Consider a model too significant if the p-value < 0.01. Every model is signfiicant except one. This is chas (Charles River dummy variable (1 if tract counds river; 0 if otherwise))

# Fit all of the predictors:
gm = lm(crim ~ ., data = Boston)
summary(gm)
```
```{r}
multivariate_beta_values = coefficients(gm)
multivariate_beta_values = multivariate_beta_values[possible_predictors[-1]] 
# Order the coefficients in the same order as the univariate coefficients 
```

```{r}
# Plot these 2 coefficients 
plot(univariable_beta_value, multivariate_beta_values)
```

```{r}
# Look for the possibility of including a non-linear term 
plot(gm)
```

```{r}
# See if we should fit a non-linear model of one variable (compare the ANOVA values):
crim = Boston[,1]
anova_f_value = c()
possible_predictors = colnames(Boston)

for (pi in 1:length(possible_predictors)) {
  if(possible_predictors[pi] == 'crim'){next}
  x = Boston[,pi]
  if(possible_predictors[pi] == 'chas'){
    F = NA
  }else{
    m_1 = lm(crim ~ x)
    m_3 = lm(crim ~ poly(x,3))
    F = anova(m_1, m_3)$F[2]
  }
  anova_f_value = c(anova_f_value, F)
}
```

```{r}
DF = data.frame(feature=colnames(Boston)[-1], f_values=anova_f_value)
DF[order(anova_f_value),]
```

```{r}
# Let's look at how the non-linear model compared to the linear model 
m_1 = lm(crim ~ medv, data=Boston)
m_3 = lm(crim ~ poly(medv,3), data = Boston)

plot(crim ~ medv, data=Boston)
lines(Boston$medv, predict(m_1), col='red', type='p')
lines(Boston$medv, predict(m_3), col='green', type='p')
```

